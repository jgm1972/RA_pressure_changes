{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77500c02-8001-4f37-ab15-5c9bb532617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "import sys, os\n",
    "from sedona.spark.sql.st_constructors import ST_Point\n",
    "from sedona.spark.sql.st_functions import GeometryType\n",
    "from sedona.spark import SedonaKepler\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import product\n",
    "from sedona.spark.geopandas import GeoDataFrame, read_parquet\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89880cb3-fce9-4ec6-b758-58b813747571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SedonaContext (or SparkSession)\n",
    "#spark = SedonaContext.builder().appName(\"ImportCSVDirectory\").getOrCreate()\n",
    "#sedona = SedonaContext.create(spark)\n",
    "# For anonymous access to public S3 buckets\n",
    "sd = (\n",
    "    SedonaContext.builder()\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.bucket.bucket-name.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sedona = SedonaContext.create(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715fd32-8fbf-4755-b1bb-f81b8140f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the directory containing your CSV files\n",
    "directory_path = \"../2024/\"\n",
    "\n",
    "# Read all CSV files from the directory into a DataFrame\n",
    "# Assuming your CSV files have a header\n",
    "#custom_schema = StructType([\n",
    "#StructField(\"STATION\", StringType(), True),\n",
    "#StructField(\"DATE\", DateType(), True),\n",
    "#StructField(\"LATITUDE\", FloatType(), True),\n",
    "#StructField(\"LONGITUDE\", FloatType(), True),\n",
    "#StructField(\"HourlyPressureChange\", FloatType(), True)\n",
    "#])\n",
    "#df = spark.read.option(\"header\", True).schema(custom_schema).format(\"csv\").load(directory_path)\n",
    "df = sd.read.option(\"header\", True).format(\"csv\").load(directory_path)\n",
    "\n",
    "\n",
    "\n",
    "# Show the DataFrame (optional)\n",
    "df.show(2)\n",
    "\n",
    "# If your CSV files contain geometry data in WKT format (e.g., in a column named \"geom_wkt\"),\n",
    "# you can convert it to a Sedona geometry column:\n",
    "# from sedona.sql.functions import ST_GeomFromText\n",
    "# df = df.withColumn(\"geometry\", ST_GeomFromText(col(\"geom_wkt\"))).drop(\"geom_wkt\")\n",
    "\n",
    "# Now you can work with the DataFrame containing all your CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f0a1a-831c-4e5c-bcea-b79f8c71105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_diff = df.select(ST_Point(col(\"LONGITUDE\"), col(\"LATITUDE\")).alias(\"GEOMETRY\"),\"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"REPORT_TYPE\", \"SOURCE\", \"HourlyDryBulbTemperature\", \"HourlyPressureChange\", \"HourlyPressureTendency\", \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \"HourlyWindDirection\", \"HourlyWindGustSpeed\", \"HourlyWindSpeed\").filter(col(\"HourlyPressureChange\").isNotNull())\n",
    "#print(df_p_diff.groupBy(\"STATION\").agg(countDistinct(\"DATE\").alias(\"HPC_count\")))\n",
    "df_p_diff.select(countDistinct('STATION')).show()\n",
    "#print(df.groupBy(\"STATION\").agg(countDistinct(\"DATE\").alias(\"FULL_count\")))\n",
    "df.select(countDistinct('STATION')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bfac7-1f27-45ae-a0da-38164d942f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import KNNQuery\n",
    "from shapely.geometry import Point\n",
    "\n",
    "spatialRDD = StructuredAdapter.toSpatialRdd(df_p_diff, \"GEOMETRY\")\n",
    "\n",
    "k = 100 ## K Nearest Neighbors\n",
    "using_index = False\n",
    "\n",
    "result = KNNQuery.SpatialKnnQuery(spatialRDD, spatial_df.first()['geometry'], k, using_index)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce6353-da19-4281-b72a-9ac3ed7a74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialRDD.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81aeb07-bc6c-4cf5-b445-e086b5ad9a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spatialRDD.spatialPartitioning(GridType.KDBTREE, 5000)\n",
    "spatialRDD.buildIndex(IndexType.RTREE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31af2f-22a2-4534-862e-2fe3c7cbc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialRDD.buildIndex(IndexType.RTREE, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bee3eb-1dd3-4933-8152-773d4dfb5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatialRDD.rawSpatialRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f669e2-120d-48ba-a93d-6e54519aa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 100\n",
    "using_index = True\n",
    "result = KNNQuery.SpatialKnnQuery(spatialRDD, spatial_df.first()['geometry'], k, using_index)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf457ef1-f1d2-41bf-8b7e-8e54bf51e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spatialRDD.buildIndex(IndexType.RTREE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e055e-3453-4f89-a42d-af97b47af02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdbb78e-fd46-4fff-b20b-2e412326e8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef3ee6-fd16-4df6-99a7-2a8046399056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Player around.  Use some of this code.\n",
    "df_pressure_diff = df.select(ST_Point(col(\"LONGITUDE\"), col(\"LATITUDE\")),\"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"REPORT_TYPE\", \"SOURCE\", \"HourlyDryBulbTemperature\", \"HourlyPressureChange\", \"HourlyPressureTendency\", \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \"HourlyWindDirection\", \"HourlyWindGustSpeed\", \"HourlyWindSpeed\").filter(col(\"HourlyPressureChange\").isNotNull()).show()\n",
    "df.filter(col(\"HourlySeaLevelPressure\").isNotNull() | col(\"HourlyStationPressure\").isNotNull()).count()\n",
    "#counts of records with different filters\n",
    "df.count()\n",
    "#130,112,717 37,352,572 77,319,947 77,210,243\n",
    "aggregated = df.groupby('STATION', 'DATE').agg({abs('HourlyPressureChange'): 'min', abs('HourlyPressureChange'): 'min'})\n",
    "df['AbsPressure'] = df['HourlyPressureChange'].abs()\n",
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn(\"absPressure\", F.abs(F.col(\"HourlyPressureChange\")))\n",
    "df.tail(10)\n",
    "df.agg(F.min('HourlyPressureChange')).show()\n",
    "df.filter(col('HourlyPressureChange').isNotNull()).withColumn(\"HourlyPressureChange\", col(\"HourlyPressureChange\").cast(FloatType())).groupby('STATION', 'DATE')\\\n",
    ".agg({abs('HourlyPressureChange'): 'min', abs('HourlyPressureChange'): 'min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0581b-f10b-4a3c-a2ce-403730052cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Overtures instread\n",
    "#import geopandas as gpd\n",
    "\n",
    "#url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "\n",
    "\n",
    "#gdf = gpd.read_file(url)\n",
    "#gdf\n",
    "#df_conus = sedona.createDataFrame(gdf[(gdf.SOV_A3=='US1') & (gdf.TYPE=='Country')][['SOVEREIGNT', 'geometry']])\n",
    "#map = SedonaKepler.create_map(df=df_conus, name=\"CONUS\")\n",
    "#map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6410458-8d6a-4a41-9b86-54d1baec2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define a coordinate grid with a 0.5-degree step\n",
    "longitude_step = 0.5\n",
    "latitude_step = 0.5\n",
    "\n",
    "longitudes = [i * longitude_step for i in range(int(-180 / longitude_step), int(180 / longitude_step) + 1)]\n",
    "latitudes = [i * latitude_step for i in range(int(-90 / latitude_step), int(90 / latitude_step) + 1)]\n",
    "\n",
    "# 3. Generate a list of all coordinate pairs\n",
    "coordinate_pairs = list(product(longitudes, latitudes))\n",
    "\n",
    "# 4. Create a Spark DataFrame from the list of coordinates\n",
    "schema = [\"longitude\", \"latitude\"]\n",
    "df_lat_lon = sedona.createDataFrame(coordinate_pairs, schema=schema)\n",
    "\n",
    "# 5. Create the Sedona geometry points\n",
    "# ST_Point takes longitude first, then latitude.\n",
    "spatial_df = df_lat_lon.withColumn(\n",
    "    \"geometry\",\n",
    "    F.expr(f\"ST_Point(longitude, latitude)\")\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "print(\"Generated spatial DataFrame:\")\n",
    "spatial_df.show(5)\n",
    "spatial_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a1cff-8493-4dba-ba6f-0763111e7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_df.first()['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2238b-0505-41e7-871c-762210b9c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boundaries = sd.read_parquet(\"s3://overturemaps-us-west-2/release/2025-09-24.0/theme=divisions/type=division_area/*.parquet\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb86ae-a1b6-487c-9bda-94a8445d7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERTURE_RELEASE = \"2025-09-24.0\"\n",
    "COUNTRY_CODES_OF_INTEREST = [\"US\"]\n",
    "SOURCE_DATA_URL = f\"s3a://overturemaps-us-west-2/release/{OVERTURE_RELEASE}/theme=divisions/type=division_area\"\n",
    "OUTPUT_FILE = \"my_super_cool_data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d695a5-6891-4368-b97a-7593a4010597",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_overlap_condition = F.arrays_overlap(\n",
    "    F.col(\"country\"),\n",
    "    F.array(*[F.lit(x.upper()) for x in COUNTRY_CODES_OF_INTEREST]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31decec6-4ea9-438e-b8ee-f514994eef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = (\n",
    "    sd.read.format(\"geoparquet\")\n",
    "    .load(SOURCE_DATA_URL)\n",
    "    .filter(col(\"country\").isin(COUNTRY_CODES_OF_INTEREST))\n",
    "    #.filter(col(\"region\")=='US-CA')\n",
    "    .filter(col(\"subtype\")=='country')\n",
    "    .withColumn(\"_overture_release_version\", F.lit(OVERTURE_RELEASE))\n",
    "    .withColumn(\"_ingest_timestamp\", F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa21f04-90c5-4ea1-b2f0-7f0f2ac5212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_geom = source_df.selectExpr(\"geometry\", \"country\")\n",
    "USA_geom.show(5)\n",
    "map = SedonaKepler.create_map(USA_geom, name=\"USA\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c38f6-3eb9-4f47-8993-1c03fd120029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for testing with just CA\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\")\n",
    "#CA_geom.show(5)\n",
    "#map1 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map1\n",
    "#\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\").filter(GeometryType(col('geometry'))=='MULTIPOLYGON')\n",
    "#CA_geom.show(5)\n",
    "#map2 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map2\n",
    "#\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\").filter(GeometryType(col('geometry'))=='POLYGON')\n",
    "#CA_geom.show(5)\n",
    "#map3 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1b4c0-c432-4cd1-9c9b-eaefc0ee5f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
