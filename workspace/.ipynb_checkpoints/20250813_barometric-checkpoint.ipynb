{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77500c02-8001-4f37-ab15-5c9bb532617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/06 20:33:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Requirements for the spark workflow\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from sedona.spark.sql.st_constructors import ST_Point\n",
    "from sedona.spark.sql.st_functions import GeometryType\n",
    "from sedona.spark import SedonaKepler\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark.geopandas import GeoDataFrame, read_parquet\n",
    "\n",
    "import sys, os\n",
    "from shapely.geometry import Point\n",
    "from itertools import product\n",
    "import sedona\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "import sedona.db\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051c97e3-67c1-4fb0-b0c8-bdaef50aefb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 20:33:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/11/06 20:33:39 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/11/06 20:33:39 WARN SimpleFunctionRegistry: The function rs_union_aggr replaced a previously registered function.\n",
      "25/11/06 20:33:39 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/11/06 20:33:39 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.common.S2Geography.Geography, which is already registered.\n",
      "25/11/06 20:33:39 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/11/06 20:33:39 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/11/06 20:33:39 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n",
      "25/11/06 20:33:39 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# For anonymous access to public S3 buckets\n",
    "#sd_cont is needed to read all the csv in\n",
    "sd_cont = (\n",
    "    SedonaContext.builder()\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.bucket.bucket-name.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "sd = SedonaContext.create(sd_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a715fd32-8fbf-4755-b1bb-f81b8140f3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the directory containing your CSV files\n",
    "directory_path = \"../2024/\"\n",
    "\n",
    "df = sd_cont.read.option(\"header\", True).format(\"csv\").load(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07873ead-019a-4b20-9d87-9cc6becc683c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_p_diff' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#I'm not going to use sedonaDB because the df is 8.7gb (from 70gb in text files)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#and I only have 12 gb memmory allocted to docker\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf_p_diff\u001b[49m\u001b[38;5;241m.\u001b[39mexplain(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_p_diff' is not defined"
     ]
    }
   ],
   "source": [
    "#I'm not going to use sedonaDB because the df is 8.7gb (from 70gb in text files)\n",
    "#and I only have 12 gb memmory allocted to docker\n",
    "df_p_diff.explain('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9f0a1a-831c-4e5c-bcea-b79f8c71105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the ST_Point after getting unique values\n",
    "df_p_diff = df.select(\"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"REPORT_TYPE\", \"SOURCE\", \"HourlyDryBulbTemperature\", \"HourlyPressureChange\", \"HourlyPressureTendency\", \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \"HourlyWindDirection\", \"HourlyWindGustSpeed\", \"HourlyWindSpeed\").filter(col(\"HourlyPressureChange\").isNotNull())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565bfac7-1f27-45ae-a0da-38164d942f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.spark import KNNQuery\n",
    "from shapely.geometry import Point\n",
    "#This has to be build with unique locations, or at least stations\n",
    "df_stations = df_p_diff.select(\"STATION\", \"LONGITUDE\", \"LATITUDE\").distinct()\n",
    "df_stations = df_stations.select(ST_Point(col(\"LONGITUDE\"), col(\"LATITUDE\")).alias(\"GEOMETRY\"), \"STATION\")\n",
    "#Don't need the RDD because I'm using queries.\n",
    "#Xi = StructuredAdapter.toSpatialRdd(df_stations, \"GEOMETRY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce6353-da19-4281-b72a-9ac3ed7a74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure out why the number of paritions isn't changing.\n",
    "#spatialRDD.analyze()\n",
    "#spatialRDD.spatialPartitioning(GridType.KDBTREE, 2500)\n",
    "#print(spatialRDD.rawSpatialRDD.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6410458-8d6a-4a41-9b86-54d1baec2453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated spatial DataFrame:\n",
      "+---------+--------+------------------+\n",
      "|longitude|latitude|          GEOMETRY|\n",
      "+---------+--------+------------------+\n",
      "|   -180.0|   -90.0|  POINT (-180 -90)|\n",
      "|   -180.0|   -89.5|POINT (-180 -89.5)|\n",
      "|   -180.0|   -89.0|  POINT (-180 -89)|\n",
      "|   -180.0|   -88.5|POINT (-180 -88.5)|\n",
      "|   -180.0|   -88.0|  POINT (-180 -88)|\n",
      "+---------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- GEOMETRY: geometry (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Define a coordinate grid with a 0.5-degree step\n",
    "longitude_step = .5\n",
    "latitude_step = .5\n",
    "\n",
    "longitudes = [i * longitude_step for i in range(int(-180 / longitude_step), int(180 / longitude_step) + 1)]\n",
    "latitudes = [i * latitude_step for i in range(int(-90 / latitude_step), int(90 / latitude_step) + 1)]\n",
    "\n",
    "# 3. Generate a list of all coordinate pairs\n",
    "coordinate_pairs = list(product(longitudes, latitudes))\n",
    "\n",
    "# 4. Create a Spark DataFrame from the list of coordinates\n",
    "schema = [\"longitude\", \"latitude\"]\n",
    "df_lat_lon = sd.createDataFrame(coordinate_pairs, schema=schema)\n",
    "\n",
    "# 5. Create the Sedona geometry points\n",
    "# ST_Point takes longitude first, then latitude.\n",
    "spatial_df = df_lat_lon.withColumn(\n",
    "    \"GEOMETRY\",\n",
    "    F.expr(f\"ST_Point(longitude, latitude)\")\n",
    ")\n",
    "X = StructuredAdapter.toSpatialRdd(spatial_df, \"GEOMETRY\")\n",
    "# Show the resulting DataFrame\n",
    "print(\"Generated spatial DataFrame:\")\n",
    "spatial_df.show(5)\n",
    "spatial_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50868ef0-6cca-4e68-922f-e0bf4d8d91d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 20:35:11 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_stations.createOrReplaceTempView(\"stations\")\n",
    "spatial_df.createOrReplaceTempView(\"interp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed62f284-b6b4-466b-8f2c-6a196c8b464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                       (0 + 10) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------------+------------------+\n",
      "|       stations_GEOM|    station|        interp_GEOM|          DISTANCE|\n",
      "+--------------------+-----------+-------------------+------------------+\n",
      "|POINT (-56.016666...|86960099999|  POINT (-56 -28.5)|16758.474764314607|\n",
      "|POINT (-56.016666...|86960099999|    POINT (-56 -29)| 38952.12593791301|\n",
      "|POINT (-56.016666...|86960099999|POINT (-56.5 -28.5)| 50058.20482966996|\n",
      "|POINT (-56.016666...|86960099999|POINT (-55.5 -28.5)|  53138.2717987768|\n",
      "|POINT (-56.016666...|86960099999|  POINT (-56.5 -29)| 61087.06711078973|\n",
      "+--------------------+-----------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 947 ms, sys: 143 ms, total: 1.09 s\n",
      "Wall time: 1min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 20:38:23 WARN Executor: Managed memory leak detected; size = 36816692 bytes, task 0.0 in stage 11.0 (TID 11203)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Adapted From https://wherobots.com/blog/introducing-knn-join-for-wherobots-and-apache-sedona/\n",
    "\n",
    "df_knn_join = sd.sql(\"\"\"\n",
    "SELECT\n",
    "        stations.GEOMETRY AS stations_GEOM,\n",
    "        stations.STATION as station,\n",
    "        interp.GEOMETRY AS interp_GEOM,\n",
    "        ST_DISTANCESPHERE(stations.GEOMETRY, interp.GEOMETRY) AS DISTANCE\n",
    "FROM stations\n",
    "JOIN interp ON ST_KNN(stations.GEOMETRY, interp.GEOMETRY, 10, FALSE)\n",
    "\"\"\")\n",
    "df_knn_join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c047e14-75b5-45c5-9204-14f9fa935466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_knn_join.write.format(\"geoparquet\").mode(\"overwrite\").save(\"./knn_join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ffd1929-210b-49dc-aa29-124c9930a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+------------------+\n",
      "|       stations_GEOM|    station|       interp_GEOM|          DISTANCE|\n",
      "+--------------------+-----------+------------------+------------------+\n",
      "|POINT (125.7 38.0...|47069099999|  POINT (125.5 38)| 17908.35967397921|\n",
      "|POINT (125.7 38.0...|47069099999|    POINT (126 38)|26540.968944462053|\n",
      "|POINT (125.7 38.0...|47069099999|POINT (125.5 38.5)| 54749.88082080413|\n",
      "|POINT (125.7 38.0...|47069099999|  POINT (126 38.5)|58126.040500040974|\n",
      "|POINT (125.7 38.0...|47069099999|POINT (125.5 37.5)| 61854.87083688271|\n",
      "+--------------------+-----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pq = sedona.read.format(\"geoparquet\").load(\"/tmp/somewhere\")\n",
    "df_pq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61bff39f-e518-4b06-83c2-247909b57980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq.createOrReplaceTempView(\"stations_knn\")\n",
    "df_p_diff.createOrReplaceTempView(\"pressure_diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f19596e-4d50-482a-87df-be8fee1a35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=====================================================>(972 + 7) / 979]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|     interp_GEOM|  interpolated_value|\n",
      "+----------------+--------------------+\n",
      "|POINT (4.5 53.5)|-0.00546123837219...|\n",
      "|  POINT (5 51.5)|-0.00391270389632...|\n",
      "|POINT (3.5 51.5)|-0.00377550987762...|\n",
      "|  POINT (5 52.5)|-0.00564268740976...|\n",
      "|  POINT (6 53.5)|-0.00628782606006...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 777 ms, sys: 89.4 ms, total: 867 ms\n",
      "Wall time: 2min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_iwd_grid = sd.sql(\"\"\"\n",
    "SELECT \n",
    "    stations_knn.interp_GEOM,\n",
    "    SUM(pressure_diff.HourlyPressureChange / POWER(stations_knn.DISTANCE, 2)) / SUM(1 / POWER(stations_knn.DISTANCE, 2)) AS interpolated_value\n",
    "    from \n",
    "      stations_knn\n",
    "    inner join \n",
    "      pressure_diff \n",
    "    on \n",
    "      stations_knn.station = pressure_diff.station\n",
    "    group by stations_knn.interp_geom\n",
    "\"\"\")\n",
    "df_iwd_grid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06dc2b7-6906-432d-a6f9-a9d272ca342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query logic\n",
    "join knn to pressure on station\n",
    "group by stations_GEOM\n",
    "calculate field like this\n",
    "SUM(pressure_diff.HourlyPressureChange / POWER(stations_knn.DISTANCE, 2)) /\n",
    "    SUM(1 / POWER(stations_knn.DISTANCE, 2)) AS interpolated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f669e2-120d-48ba-a93d-6e54519aa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k = 5\n",
    "using_index = False\n",
    "#result = KNNQuery.SpatialKnnQuery(spatialRDD, Point(-122, 47.5), k, using_index)\n",
    "result = KNNQuery.SpatialKnnQuery(Xi, X, k, using_index)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef3ee6-fd16-4df6-99a7-2a8046399056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Player around.  Use some of this code.\n",
    "df_pressure_diff = df.select(ST_Point(col(\"LONGITUDE\"), col(\"LATITUDE\")),\"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"REPORT_TYPE\", \"SOURCE\", \"HourlyDryBulbTemperature\", \"HourlyPressureChange\", \"HourlyPressureTendency\", \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \"HourlyWindDirection\", \"HourlyWindGustSpeed\", \"HourlyWindSpeed\").filter(col(\"HourlyPressureChange\").isNotNull()).show()\n",
    "df.filter(col(\"HourlySeaLevelPressure\").isNotNull() | col(\"HourlyStationPressure\").isNotNull()).count()\n",
    "#counts of records with different filters\n",
    "df.count()\n",
    "#130,112,717 37,352,572 77,319,947 77,210,243\n",
    "aggregated = df.groupby('STATION', 'DATE').agg({abs('HourlyPressureChange'): 'min', abs('HourlyPressureChange'): 'min'})\n",
    "df['AbsPressure'] = df['HourlyPressureChange'].abs()\n",
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn(\"absPressure\", F.abs(F.col(\"HourlyPressureChange\")))\n",
    "df.tail(10)\n",
    "df.agg(F.min('HourlyPressureChange')).show()\n",
    "df.filter(col('HourlyPressureChange').isNotNull()).withColumn(\"HourlyPressureChange\", col(\"HourlyPressureChange\").cast(FloatType())).groupby('STATION', 'DATE')\\\n",
    ".agg({abs('HourlyPressureChange'): 'min', abs('HourlyPressureChange'): 'min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0581b-f10b-4a3c-a2ce-403730052cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Overtures instread\n",
    "#import geopandas as gpd\n",
    "\n",
    "#url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "\n",
    "\n",
    "#gdf = gpd.read_file(url)\n",
    "#gdf\n",
    "#df_conus = sedona.createDataFrame(gdf[(gdf.SOV_A3=='US1') & (gdf.TYPE=='Country')][['SOVEREIGNT', 'geometry']])\n",
    "#map = SedonaKepler.create_map(df=df_conus, name=\"CONUS\")\n",
    "#map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a1cff-8493-4dba-ba6f-0763111e7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_df.first()['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2238b-0505-41e7-871c-762210b9c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boundaries = sd.read_parquet(\"s3://overturemaps-us-west-2/release/2025-09-24.0/theme=divisions/type=division_area/*.parquet\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb86ae-a1b6-487c-9bda-94a8445d7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERTURE_RELEASE = \"2025-09-24.0\"\n",
    "COUNTRY_CODES_OF_INTEREST = [\"US\"]\n",
    "SOURCE_DATA_URL = f\"s3a://overturemaps-us-west-2/release/{OVERTURE_RELEASE}/theme=divisions/type=division_area\"\n",
    "OUTPUT_FILE = \"my_super_cool_data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d695a5-6891-4368-b97a-7593a4010597",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_overlap_condition = F.arrays_overlap(\n",
    "    F.col(\"country\"),\n",
    "    F.array(*[F.lit(x.upper()) for x in COUNTRY_CODES_OF_INTEREST]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31decec6-4ea9-438e-b8ee-f514994eef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = (\n",
    "    sd.read.format(\"geoparquet\")\n",
    "    .load(SOURCE_DATA_URL)\n",
    "    .filter(col(\"country\").isin(COUNTRY_CODES_OF_INTEREST))\n",
    "    #.filter(col(\"region\")=='US-CA')\n",
    "    .filter(col(\"subtype\")=='country')\n",
    "    .withColumn(\"_overture_release_version\", F.lit(OVERTURE_RELEASE))\n",
    "    .withColumn(\"_ingest_timestamp\", F.current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa21f04-90c5-4ea1-b2f0-7f0f2ac5212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_geom = source_df.selectExpr(\"geometry\", \"country\")\n",
    "USA_geom.show(5)\n",
    "map = SedonaKepler.create_map(USA_geom, name=\"USA\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c38f6-3eb9-4f47-8993-1c03fd120029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for testing with just CA\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\")\n",
    "#CA_geom.show(5)\n",
    "#map1 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map1\n",
    "#\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\").filter(GeometryType(col('geometry'))=='MULTIPOLYGON')\n",
    "#CA_geom.show(5)\n",
    "#map2 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map2\n",
    "#\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\").filter(GeometryType(col('geometry'))=='POLYGON')\n",
    "#CA_geom.show(5)\n",
    "#map3 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1b4c0-c432-4cd1-9c9b-eaefc0ee5f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
