{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77500c02-8001-4f37-ab15-5c9bb532617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements for the spark workflow\n",
    "from sedona.spark import *\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n",
    "from sedona.spark.sql.st_constructors import ST_Point\n",
    "from sedona.spark.sql.st_functions import GeometryType, ST_XMin, ST_YMin, ST_XMax, ST_YMax\n",
    "from sedona.spark import SedonaKepler\n",
    "from pyspark.sql import functions as F\n",
    "from sedona.spark.geopandas import GeoDataFrame, read_parquet\n",
    "from sedona.sql import st_predicates\n",
    "\n",
    "import sys, os\n",
    "from shapely.geometry import Point\n",
    "from itertools import product\n",
    "import sedona\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "import sedona.db\n",
    "import numpy as np\n",
    "import leafmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051c97e3-67c1-4fb0-b0c8-bdaef50aefb8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/07 20:37:30 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/11/07 20:37:32 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/11/07 20:37:32 WARN SimpleFunctionRegistry: The function rs_union_aggr replaced a previously registered function.\n",
      "25/11/07 20:37:32 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/11/07 20:37:32 WARN UDTRegistration: Cannot register UDT for org.apache.sedona.common.S2Geography.Geography, which is already registered.\n",
      "25/11/07 20:37:32 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/11/07 20:37:32 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/11/07 20:37:32 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n",
      "25/11/07 20:37:32 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# For anonymous access to public S3 buckets\n",
    "#sd_cont is needed to read all the csv in\n",
    "sd_cont = (\n",
    "    SedonaContext.builder()\n",
    "    .config(\n",
    "        \"spark.hadoop.fs.s3a.bucket.bucket-name.aws.credentials.provider\",\n",
    "        \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "sd = SedonaContext.create(sd_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a715fd32-8fbf-4755-b1bb-f81b8140f3da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the directory containing your CSV files\n",
    "directory_path = \"../2024/\"\n",
    "\n",
    "df = sd_cont.read.option(\"header\", True).format(\"csv\").load(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9f0a1a-831c-4e5c-bcea-b79f8c71105c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'sedona' has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Setting the ST_Point after getting unique values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(ST_Point(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLONGITUDE\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGEOMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLATITUDE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLONGITUDE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELEVATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREPORT_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOURCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyDryBulbTemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyPressureChange\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyPressureTendency\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlySeaLevelPressure\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyStationPressure\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyWindDirection\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyWindGustSpeed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyWindSpeed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourlyPressureChange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeoparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./p_diff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df_p_diff_pq \u001b[38;5;241m=\u001b[39m \u001b[43msedona\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeoparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./p_diff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'sedona' has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "#Setting the ST_Point after getting unique values\n",
    "df.select(ST_Point(col(\"LONGITUDE\"), col(\"LATITUDE\")).alias(\"GEOMETRY\"), \"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"REPORT_TYPE\", \n",
    "          \"SOURCE\", \"HourlyDryBulbTemperature\", \"HourlyPressureChange\", \"HourlyPressureTendency\", \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \n",
    "          \"HourlyWindDirection\", \"HourlyWindGustSpeed\", \"HourlyWindSpeed\").filter(col(\"HourlyPressureChange\").isNotNull()).write.format(\"geoparquet\").mode(\"overwrite\").save(\"./p_diff\")\n",
    "df_p_diff_pq = sd.read.format(\"geoparquet\").load(\"./p_diff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07873ead-019a-4b20-9d87-9cc6becc683c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#I'm not going to use sedonaDB because the df is 8.7gb (from 70gb in text files)\n",
    "#and I only have 12 gb memmory allocted to docker\n",
    "df_p_diff_pq.explain('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565bfac7-1f27-45ae-a0da-38164d942f31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            GEOMETRY|\n",
      "+--------------------+\n",
      "|  POINT (14.35 49.2)|\n",
      "|POINT (136.9 37.3...|\n",
      "|POINT (-60.983333...|\n",
      "|   POINT (1.4 44.75)|\n",
      "|POINT (139.366666...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/07 20:44:07 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9156"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This has to be build with unique locations, or at least stations\n",
    "df_p_diff_pq.select(\"GEOMETRY\").distinct().write.format(\"geoparquet\").mode(\"overwrite\").save(\"./stations\")\n",
    "df_stations_pq = sd.read.format(\"geoparquet\").load(\"./stations\")\n",
    "df_stations_pq.show(5)\n",
    "df_stations_pq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5cb86ae-a1b6-487c-9bda-94a8445d7d96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            geometry|country|\n",
      "+--------------------+-------+\n",
      "|MULTIPOLYGON (((-...|     US|\n",
      "|MULTIPOLYGON (((-...|     US|\n",
      "|MULTIPOLYGON (((-...|     US|\n",
      "|MULTIPOLYGON (((-...|     US|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=====================================================>  (25 + 1) / 26]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3667e5e943f246b6afe80a15add98f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'USA': {'index': [0, 1, 2, 3], 'columns': ['geometry', 'country'], 'data': [['MULTIPOLYGON (((-…"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/07 20:45:54 ERROR FileFormatWriter: Aborting job ba48bf7d-d73a-434f-a1a9-15091e458d07.\n",
      "java.io.FileNotFoundException: File file:/opt/workspace/ra/workspace/USA_geom/_temporary/0 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\n",
      "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\n",
      "\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    }
   ],
   "source": [
    "OVERTURE_RELEASE = \"2025-09-24.0\"\n",
    "COUNTRY_CODES_OF_INTEREST = [\"US\"]\n",
    "SOURCE_DATA_URL = f\"s3a://overturemaps-us-west-2/release/{OVERTURE_RELEASE}/theme=divisions/type=division_area\"\n",
    "OUTPUT_FILE = \"my_super_cool_data.parquet\"\n",
    "\n",
    "country_overlap_condition = F.arrays_overlap(\n",
    "    F.col(\"country\"),\n",
    "    F.array(*[F.lit(x.upper()) for x in COUNTRY_CODES_OF_INTEREST]),\n",
    ")\n",
    "\n",
    "source_df = (\n",
    "    sd.read.format(\"geoparquet\")\n",
    "    .load(SOURCE_DATA_URL)\n",
    "    .filter(col(\"country\").isin(COUNTRY_CODES_OF_INTEREST))\n",
    "    #.filter(col(\"region\")=='US-CA')\n",
    "    .filter(col(\"subtype\")=='country')\n",
    "    .withColumn(\"_overture_release_version\", F.lit(OVERTURE_RELEASE))\n",
    "    .withColumn(\"_ingest_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "source_df.selectExpr(\"geometry\", \"country\").write.format(\"geoparquet\").mode(\"overwrite\").save(\"./USA_geom\")\n",
    "USA_geom_pq = sd.read.format(\"geoparquet\").load(\"./USA_geom\")\n",
    "USA_geom_pq.show(5)\n",
    "map = SedonaKepler.create_map(USA_geom_pq, name=\"USA\")\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6410458-8d6a-4a41-9b86-54d1baec2453",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated spatial DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------------+--------------------+-------+\n",
      "|longitude|latitude|        interp_geom|            geometry|country|\n",
      "+---------+--------+-------------------+--------------------+-------+\n",
      "|   -179.0|    51.5|  POINT (-179 51.5)|MULTIPOLYGON (((-...|     US|\n",
      "|   -179.0|    51.5|  POINT (-179 51.5)|MULTIPOLYGON (((-...|     US|\n",
      "|   -179.0|    52.0|    POINT (-179 52)|MULTIPOLYGON (((-...|     US|\n",
      "|   -179.0|    52.0|    POINT (-179 52)|MULTIPOLYGON (((-...|     US|\n",
      "|   -178.5|    28.5|POINT (-178.5 28.5)|MULTIPOLYGON (((-...|     US|\n",
      "+---------+--------+-------------------+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- interp_geom: geometry (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/opt/workspace/ra/workspace/.interp_points.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:32\u001b[0m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/opt/workspace/ra/workspace/.interp_points."
     ]
    }
   ],
   "source": [
    "# 2. Define a coordinate grid with a 0.5-degree step\n",
    "longitude_step = .5\n",
    "latitude_step = .5\n",
    "\n",
    "longitudes = [i * longitude_step for i in range(int(-180 / longitude_step), int(180 / longitude_step) + 1)]\n",
    "latitudes = [i * latitude_step for i in range(int(-90 / latitude_step), int(90 / latitude_step) + 1)]\n",
    "\n",
    "# 3. Generate a list of all coordinate pairs\n",
    "coordinate_pairs = list(product(longitudes, latitudes))\n",
    "\n",
    "# 4. Create a Spark DataFrame from the list of coordinates\n",
    "schema = [\"longitude\", \"latitude\"]\n",
    "df_lat_lon = sd.createDataFrame(coordinate_pairs, schema=schema)\n",
    "\n",
    "# 5. Create the Sedona geometry points\n",
    "# ST_Point takes longitude first, then latitude.\n",
    "spatial_df = df_lat_lon.withColumn(\n",
    "    \"interp_geom\",\n",
    "    F.expr(f\"ST_Point(longitude, latitude)\")\n",
    ")\n",
    "# Show the resulting DataFrame clipped to USA boundaries\n",
    "print(\"Generated spatial DataFrame:\")\n",
    "df_usa_points = spatial_df.join(\n",
    "    USA_geom_pq,\n",
    "    st_predicates.ST_Within(spatial_df[\"interp_geom\"], USA_geom_pq[\"geometry\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "# Show the results\n",
    "df_usa_points.show(5)\n",
    "df_usa_points.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a50430-1e71-40f8-837a-d6baf083a054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/07 20:51:47 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "25/11/07 20:51:47 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "25/11/07 20:51:47 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "25/11/07 20:51:47 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n",
      "25/11/07 20:51:47 WARN GeoParquetFileFormat: GeoParquet currently does not support vectorized reader. Falling back to parquet-mr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      interp_geom|\n",
      "+-----------------+\n",
      "|  POINT (-108 32)|\n",
      "|  POINT (-108 32)|\n",
      "|  POINT (-108 32)|\n",
      "|  POINT (-108 32)|\n",
      "|POINT (-108 32.5)|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- interp_geom: geometry (nullable = true)\n",
      "\n",
      "18636\n",
      "260281\n",
      "CPU times: user 29.2 ms, sys: 6.89 ms, total: 36.1 ms\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_usa_points.drop(*[\"longitude\", \"latitude\", \"geometry\", \"country\"]).write.format(\"geoparquet\").mode(\"overwrite\").save(\"./interp_points\")\n",
    "df_interp_pq = sd.read.format(\"geoparquet\").load(\"./interp_points\")\n",
    "df_interp_pq.show(5)\n",
    "df_interp_pq.printSchema()\n",
    "print(df_interp_pq.count())\n",
    "print(spatial_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50868ef0-6cca-4e68-922f-e0bf4d8d91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations_pq.createOrReplaceTempView(\"stations\")\n",
    "df_interp_pq.createOrReplaceTempView(\"interp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed62f284-b6b4-466b-8f2c-6a196c8b464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------+\n",
      "|     stations_GEOM|       interp_GEOM|         DISTANCE|\n",
      "+------------------+------------------+-----------------+\n",
      "|POINT (14.35 49.2)|POINT (-67.5 45.5)|5869061.575762225|\n",
      "|POINT (14.35 49.2)|POINT (-67.5 45.5)|5869061.575762225|\n",
      "|POINT (14.35 49.2)|POINT (-67.5 45.5)|5869061.575762225|\n",
      "|POINT (14.35 49.2)|POINT (-67.5 45.5)|5869061.575762225|\n",
      "|POINT (14.35 49.2)|  POINT (-67.5 45)|5901613.263144317|\n",
      "+------------------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 6.75 ms, sys: 4.69 ms, total: 11.4 ms\n",
      "Wall time: 937 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Adapted From https://wherobots.com/blog/introducing-knn-join-for-wherobots-and-apache-sedona/\n",
    "df_knn_join = sd.sql(\"\"\"\n",
    "SELECT\n",
    "    stations.GEOMETRY AS stations_GEOM,\n",
    "    interp.interp_GEOM,\n",
    "    ST_DISTANCESPHERE(stations.GEOMETRY, interp.interp_GEOM) AS DISTANCE\n",
    "FROM \n",
    "    stations\n",
    "JOIN \n",
    "    interp \n",
    "ON \n",
    "    ST_KNN(stations.GEOMETRY, interp.interp_GEOM, 10, FALSE)\n",
    "\"\"\")\n",
    "df_knn_join.show(5)\n",
    "df_knn_join.write.format(\"geoparquet\").mode(\"overwrite\").save(\"./knn_join\")\n",
    "df_knn_pq = sd.read.format(\"geoparquet\").load(\"./knn_join\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61bff39f-e518-4b06-83c2-247909b57980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_diff_pq.createOrReplaceTempView(\"pressure_diff\")\n",
    "df_knn_pq.createOrReplaceTempView(\"stations_knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f19596e-4d50-482a-87df-be8fee1a35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------+\n",
      "|       interp_GEOM| interpolated_value|          avg_dist|\n",
      "+------------------+-------------------+------------------+\n",
      "| POINT (-112 33.5)|0.03225291529728004|18833.823674307456|\n",
      "|POINT (-75.5 44.5)|0.03262977226930969|185723.04136612354|\n",
      "|    POINT (-89 48)|0.03342035015265212| 583809.3943930377|\n",
      "|  POINT (-78 43.5)|0.03257088761273111|  169648.553352226|\n",
      "| POINT (-111 39.5)| 0.0248577929465244|24208.072334796652|\n",
      "+------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------------------+\n",
      "|       interp_GEOM| interpolated_value|          avg_dist|\n",
      "+------------------+-------------------+------------------+\n",
      "| POINT (-112 33.5)|0.03225291529728004|18833.823674307456|\n",
      "|POINT (-75.5 44.5)|0.03262977226930969|185723.04136612354|\n",
      "|    POINT (-89 48)|0.03342035015265212| 583809.3943930377|\n",
      "|  POINT (-78 43.5)|0.03257088761273111|  169648.553352226|\n",
      "| POINT (-111 39.5)| 0.0248577929465244|24208.072334796652|\n",
      "+------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 49.5 ms, sys: 10.7 ms, total: 60.2 ms\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_iwd_grid = sd.sql(\"\"\"\n",
    "SELECT \n",
    "    stations_knn.interp_GEOM,\n",
    "    SUM(ABS(pressure_diff.HourlyPressureChange) / POWER(stations_knn.DISTANCE, 2)) / SUM(1 / POWER(stations_knn.DISTANCE, 2)) AS interpolated_value,\n",
    "    AVG(stations_knn.DISTANCE) as avg_dist\n",
    "    from \n",
    "      stations_knn\n",
    "    inner join \n",
    "      pressure_diff \n",
    "    on \n",
    "      stations_knn.stations_GEOM = pressure_diff.GEOMETRY\n",
    "    group by stations_knn.interp_geom\n",
    "\"\"\")\n",
    "df_iwd_grid.write.format(\"geoparquet\").mode(\"overwrite\").save(\"./knn_join\")\n",
    "df_iwd_pq = sd.read.format(\"geoparquet\").load(\"./knn_join\")\n",
    "df_iwd_pq.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4597d93a-84c4-4d1e-ac86-9d242a88ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iwd_pq.createOrReplaceTempView(\"iwd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1243f58-3551-476a-b1a6-632b89a94eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-----+\n",
      "| x_min|x_max|y_min|y_max|\n",
      "+------+-----+-----+-----+\n",
      "|-178.5|-67.5| 19.0| 71.5|\n",
      "+------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_min_max = sd.sql(\"\"\"\n",
    "SELECT\n",
    "    Min(ST_XMin(interp_GEOM)) as x_min,\n",
    "    Max(ST_XMax(interp_GEOM)) as x_max,\n",
    "    Min(ST_YMin(interp_GEOM)) as y_min,\n",
    "    Max(ST_YMax(interp_GEOM)) as y_max\n",
    "from iwd\n",
    "WHERE ST_XMax(interp_GEOM) < 0\n",
    "\n",
    "\"\"\")\n",
    "df_min_max.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d06dc2b7-6906-432d-a6f9-a9d272ca342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- raster: raster (nullable = true)\n",
      "\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|raster                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|GridCoverage2D[\"genericCoverage\", GeneralBounds[(-178.75, 45.25), (-122.75, 71.75)], DefaultEngineeringCRS[\"Generic cartesian 2D\"]]\\n│   RenderedSampleDimension(\"genericCoverage\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\\n│     ‣ Category(\"\":[-1.7976931348623157E308 ... 1.7976931348623157E308])\\n└ Image=RenderedImageAdapter[]\\n|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define parameters  This should be .25 deg off of the points so there is one point per cell\n",
    "offset_x = df_min_max.first()['x_min'] -0.25\n",
    "offset_y = df_min_max.first()['y_max'] +0.25\n",
    "cell_size = 0.5\n",
    "width = df_min_max.first()['x_max'] - df_min_max.first()['x_min'] + 1\n",
    "height = df_min_max.first()['y_max'] - df_min_max.first()['y_min'] + 1\n",
    "srid = 4326 # Example SRID (WGS84)\n",
    "num_bands = 1 # Number of bands\n",
    "pixel_type = 'D' # Pixel type, e.g., 'D' for Double\n",
    "\n",
    "# Create an empty raster using SQL expression\n",
    "raster_df = sd.sql(f\"\"\"\n",
    "    SELECT RS_MakeEmptyRaster({num_bands}, '{pixel_type}', {width}, {height}, {offset_x}, {offset_y}, {cell_size}) AS raster\n",
    "\"\"\")\n",
    "\n",
    "# Show schema to verify\n",
    "raster_df.printSchema()\n",
    "raster_df.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef3ee6-fd16-4df6-99a7-2a8046399056",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Player around.  Use some of this code.\n",
    "df_pressure_diff = df.select(ST_Point(col(\"LONGITUDE\"), col(\"LATITUDE\")),\"STATION\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"NAME\", \"REPORT_TYPE\", \"SOURCE\", \"HourlyDryBulbTemperature\", \"HourlyPressureChange\", \"HourlyPressureTendency\", \"HourlySeaLevelPressure\", \"HourlyStationPressure\", \"HourlyWindDirection\", \"HourlyWindGustSpeed\", \"HourlyWindSpeed\").filter(col(\"HourlyPressureChange\").isNotNull()).show()\n",
    "df.filter(col(\"HourlySeaLevelPressure\").isNotNull() | col(\"HourlyStationPressure\").isNotNull()).count()\n",
    "#counts of records with different filters\n",
    "df.count()\n",
    "#130,112,717 37,352,572 77,319,947 77,210,243\n",
    "aggregated = df.groupby('STATION', 'DATE').agg({abs('HourlyPressureChange'): 'min', abs('HourlyPressureChange'): 'min'})\n",
    "df['AbsPressure'] = df['HourlyPressureChange'].abs()\n",
    "from pyspark.sql import functions as F\n",
    "df = df.withColumn(\"absPressure\", F.abs(F.col(\"HourlyPressureChange\")))\n",
    "df.tail(10)\n",
    "df.agg(F.min('HourlyPressureChange')).show()\n",
    "df.filter(col('HourlyPressureChange').isNotNull()).withColumn(\"HourlyPressureChange\", col(\"HourlyPressureChange\").cast(FloatType())).groupby('STATION', 'DATE')\\\n",
    ".agg({abs('HourlyPressureChange'): 'min', abs('HourlyPressureChange'): 'min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0581b-f10b-4a3c-a2ce-403730052cc9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Using Overtures instread\n",
    "#import geopandas as gpd\n",
    "\n",
    "#url = \"https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\"\n",
    "\n",
    "\n",
    "#gdf = gpd.read_file(url)\n",
    "#gdf\n",
    "#df_conus = sedona.createDataFrame(gdf[(gdf.SOV_A3=='US1') & (gdf.TYPE=='Country')][['SOVEREIGNT', 'geometry']])\n",
    "#map = SedonaKepler.create_map(df=df_conus, name=\"CONUS\")\n",
    "#map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a1cff-8493-4dba-ba6f-0763111e7706",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spatial_df.first()['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c38f6-3eb9-4f47-8993-1c03fd120029",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Used for testing with just CA\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\")\n",
    "#CA_geom.show(5)\n",
    "#map1 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map1\n",
    "#\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\").filter(GeometryType(col('geometry'))=='MULTIPOLYGON')\n",
    "#CA_geom.show(5)\n",
    "#map2 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map2\n",
    "#\n",
    "#CA_geom = source_df.selectExpr(\"geometry\", \"region\").filter(GeometryType(col('geometry'))=='POLYGON')\n",
    "#CA_geom.show(5)\n",
    "#map3 = SedonaKepler.create_map(CA_geom, name=\"CA\")\n",
    "#map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1b4c0-c432-4cd1-9c9b-eaefc0ee5f77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
